{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ead63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "from skimage.measure import block_reduce\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f298c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = Path('/kaggle/input/sroie-single-line-syn-upd/syn_img/syn_img')\n",
    "labels_path = Path('/kaggle/input/sroie-single-line-syn-upd/text/text')\n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for p in tqdm(glob('/kaggle/input/sroie-single-line-syn-upd/text/text/*/*.json')):\n",
    "    p = Path(p)\n",
    "    subfolder = p.parts[-2]\n",
    "    stem = p.stem\n",
    "    \n",
    "    with open(labels_path / subfolder / f'{stem}.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "    \n",
    "    for i in range(5):\n",
    "        image = cv.imread(str(images_path / subfolder / f'{stem}_{i}.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    \n",
    "print(\"Number of images found: \", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b32246",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    if 'В' in labels[i]:\n",
    "        labels[i] = labels[i].replace('В', 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520afe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set([c for s in labels for c in s]))\n",
    "\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title(labels[0])\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_labels = [len(x) for x in labels]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(len_labels, bins=50)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99036063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of any captcha in the dataset\n",
    "max_label_len = 32\n",
    "print('Maximum length of labels:', max_label_len)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if len(labels[i]) < max_label_len:\n",
    "        labels[i] += ' ' * (max_label_len - len(labels[i]))\n",
    "    else:\n",
    "        labels[i] = labels[i][:max_label_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90227d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=characters, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=characters, mask_token=None, invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9df5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired image dimensions\n",
    "img_width = 256\n",
    "img_height = 32\n",
    "\n",
    "images = [cv.resize(img, (img_width, img_height)) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample(image, label):\n",
    "    # 1. Convert grayscale image to 3-dimensional tensor\n",
    "    image = tf.reshape(image, [image.shape[0], image.shape[1], 1])\n",
    "    # 2. Convert to float32 in [0, 1] range\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # 3. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    # 4. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 5. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": image, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training and validation\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "#                   Create Database objects\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4549b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                   Visualize the data\n",
    "\n",
    "_, ax = plt.subplots(4, 4, figsize=(10, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    imgs = batch[\"image\"]\n",
    "    lbls = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img = (imgs[i] * 255).numpy().astype(\"uint8\")\n",
    "        lbl = tf.strings.reduce_join(num_to_char(lbls[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(lbl, size=6)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super(CTCLayer, self).__init__(name=name)\n",
    "        super(CTCLayer, self).__init__(**kwargs)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CTCLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c08e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=max_label_len + 1, dtype=\"float32\")\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv1\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2, name=\"dropout\")(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25), name=\"bidirectional1\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25), name=\"bidirectional2\")(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(\n",
    "        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1799a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                   Training\n",
    "\n",
    "epochs = 10\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_dataset,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save('sroie-single-line-syn-10.h5')\n",
    "\n",
    "with open('history-10.json', 'w') as f:\n",
    "    json.dump(history.history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17cd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
