{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "image_dir = \"D:/aircraft dataset/Images\"\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory containing the XML annotations\n",
    "annotation_dir = 'D:/aircraft dataset/Annotations/Horizontal Bounding Boxes'\n",
    "\n",
    "\n",
    "# List to store the image paths and annotations\n",
    "croped_images = []\n",
    "#bounding_boxes = []\n",
    "cropedd_labels = []\n",
    "zeros = []\n",
    "# Iterate over the XML files in the annotation directory\n",
    "for filename in os.listdir(annotation_dir):\n",
    "    if filename.endswith('.xml'):\n",
    "        xml_path = os.path.join(annotation_dir, filename)\n",
    "        image_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "\n",
    "\n",
    "        # Read the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract the image size information\n",
    "        size_element = root.find('size')\n",
    "        width = int(size_element.find('width').text)\n",
    "        height = int(size_element.find('height').text)\n",
    "        if width == 0 or height == 0:\n",
    "            zeros.append(image_filename)\n",
    "            continue\n",
    "        image_path = os.path.join(image_dir,\"JPEGImages\", image_filename)\n",
    "        image_path = image_path.replace('\\\\', '/')\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        print(image_path)\n",
    "        # Extract annotation information for each object\n",
    "        \n",
    "        class_label = obj.find('name').text\n",
    "\n",
    "        # Append the bounding box coordinates to the list\n",
    "\n",
    "        if image_croped.size == 0:\n",
    "            print(image_path)\n",
    "            continue\n",
    "        new_image_size = (120, 120)\n",
    "        image = cv2.resize(image, new_image_size, interpolation=cv2.INTER_LINEAR)  # Resize the image to match the input size of InceptionV3\n",
    "        image = preprocess_input(np.array(image))\n",
    "\n",
    "        croped_images.append(image)\n",
    "        cropedd_labels.append(int(class_label[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming `images_array` is the NumPy array of images and `labels` is the corresponding array of labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(croped_images, cropedd_labels, test_size=0.2, random_state=42)\n",
    "train_images = np.array(train_images)\n",
    "test_images = np.array(test_images)\n",
    "train_labels = np.array(train_labels) \n",
    "test_labels = np.array(test_labels)\n",
    "# Check the shapes of the split datasets\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Testing images shape:\", test_images.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Testing labels shape:\", test_labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    data_augmentation = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(64,64,3)),\n",
    "    tf.keras.layers.RandomZoom(0.07),\n",
    "    tf.keras.layers.RandomRotation(180) \n",
    "  ]\n",
    ")\n",
    "    model = tf.keras.models.Sequential([\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(450, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.65),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "   \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                    optimizer=opt,\n",
    "                    metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "x = np.arange(epochs) + 1\n",
    "history_ = model.fit(x_train, y_train, epochs=epochs, validation_split=.20, batch_size=64)\n",
    "plt.plot(x,history_.history['acc'])\n",
    "plt.plot(x,history_.history['val_acc'])\n",
    "plt.xlabel(\"Epochs\");\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['Training Accuacy', 'Cross Validation Accuracy']);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,history_.history['loss'])\n",
    "ax.plot(x,history_.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\");\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Training Loss', 'Cross Validation Loss']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install yolov5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5a0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f16a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85658d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "image_path = 'D:/aircraft dataset/Images/JPEGImages/13.jpg'\n",
    "image = Image.open(image_path)\n",
    "#image =np.array( image)\n",
    "# Perform detection\n",
    "# Set the class label for aircraft\n",
    "# Set the desired class number\n",
    "desired_class = 4\n",
    "# Perform detection\n",
    "results = model(image)\n",
    "# Filter the results based on the class number\n",
    "filtered_results = results.pred[0][results.pred[0][:, -1] == desired_class]\n",
    "\n",
    "# Access the bounding box coordinates\n",
    "bboxes = filtered_results[:, :4]\n",
    "# Access the bounding box coordinates and class labels\n",
    "#bboxes = filtered_results.xyxy[0].numpy()[:, :4]\n",
    "\n",
    "\n",
    "# Enable inline plotting in Google Colab\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Hide the axes and labels\n",
    "ax.axis('off')\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(image)\n",
    "\n",
    "# Create a rectangle patch for each bounding box and add it to the axes\n",
    "for box in bboxes:\n",
    "    # Extract the box coordinates\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    print(box)\n",
    "    # Create a rectangle patch\n",
    "    rect = patches.Rectangle((int(x_min), int(y_min)), int(x_max) - int(x_min), int(y_max) - int(y_min),\n",
    "                             linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "    # Add the rectangle patch to the axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('C:/Users/Тетяна/Desktop')\n",
    "for box in bboxes:\n",
    "    # Extract the box coordinates\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    #image =Image.open(image_path)\n",
    "    \n",
    "    #image_croped = image[ymin:ymax, xmin:xmax]\n",
    "    image_croped = image.crop((int(xmin.item()), int(ymin.item()), int(xmax.item()), int(ymax.item())))\n",
    "    image_croped = cv2.resize(image_croped, (130,130), interpolation=cv2.INTER_LINEAR)  # Resize the image to match the input size of InceptionV3\n",
    "    image_croped = preprocess_input(np.array(image_croped))\n",
    "    results = model(image)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdf1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
